{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adaf128e-3f2a-43fd-a552-d027f231ee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1e4a906-baeb-4a39-a768-2e8a1e5dbc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras._tf_keras.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab6d5186-f8d2-4b54-a43d-615ab960fcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen= ImageDataGenerator(rescale=1/255,\n",
    "                                  shear_range=0.2,\n",
    "                                  zoom_range=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6b1ed84-abae-49bf-bc6a-331dbbb57818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8048 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set=train_datagen.flow_from_directory(r\"C:\\Users\\sonia\\Downloads\\training_set-20250108T103634Z-001\\training_set\",\n",
    "                                     target_size=(64,64),\n",
    "                                     class_mode='binary')\n",
    "                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3603b547-edc2-43f2-86d8-77131821c2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2fc04b85-ec0e-4cd7-ad59-1fc76cb320f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen= ImageDataGenerator(rescale=1/255)\n",
    "test_set=test_datagen.flow_from_directory(r\"C:\\Users\\sonia\\Downloads\\test_set-20250108T103639Z-001\\test_set\",\n",
    "                                     target_size=(64,64),\n",
    "                                     class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5180103e-aa28-48a2-a325-7853cf178b46",
   "metadata": {},
   "source": [
    "# Modelling-Convolution Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb7cf4e-ded1-42d9-aab5-dcfbd5969295",
   "metadata": {},
   "source": [
    "initialising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d729f014-1243-4a70-9809-cc2e0921a14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "classifier=Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b56dc0d-c156-4912-aa24-4e8f2142df7b",
   "metadata": {},
   "source": [
    "step-1 convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f5f4493-be7a-4f2a-bca3-111ec0e4cde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sonia\\anaconda4\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D\n",
    "classifier.add(Conv2D(input_shape=[64,64,3],\n",
    "                      filters=32,kernel_size=3,activation='relu'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005dae2d-3b52-4041-9e44-3e2e6da90ad8",
   "metadata": {},
   "source": [
    "step-2 maxpooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e70c1344-a606-4d59-9639-e13f5d74b4f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Maxpooling2D' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MaxPooling2D\n\u001b[1;32m----> 2\u001b[0m classifier\u001b[38;5;241m.\u001b[39madd(Maxpooling2D(pool_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,strides\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Maxpooling2D' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.layers import MaxPooling2D\n",
    "classifier.add(Maxpooling2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4c76d5-4118-4e77-aabc-d1eee4afa3ce",
   "metadata": {},
   "source": [
    "Step-3 Flatterning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f2336853-2bab-4d1b-b94c-9609c4c4c1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2e857e-22b0-49dc-aa30-14aa921a9483",
   "metadata": {},
   "source": [
    "step-4 Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b59a750f-eaf2-48d1-9304-d1c12c7d40cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "#hidden layer with 128 neurons\n",
    "classifier.add(Dense(units=127,activation='relu'))\n",
    "\n",
    "#Output layer with 1 neuron\n",
    "classifier.add(Dense(units=1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b993f918-36f4-44f8-8bbe-02ad2f4104f4",
   "metadata": {},
   "source": [
    "Training the CNN model with train and testing the model with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "61983507-784b-4eb0-a104-d9a331b84dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer='adam',\n",
    "                   loss='binary_crossentropy',\n",
    "                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cc87c11f-2202-4269-9c67-ad9520c00549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 592ms/step - accuracy: 0.5128 - loss: 1.0138"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sonia\\anaconda4\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 701ms/step - accuracy: 0.5129 - loss: 1.0128 - val_accuracy: 0.5915 - val_loss: 0.6662\n",
      "Epoch 2/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 293ms/step - accuracy: 0.6345 - loss: 0.6385 - val_accuracy: 0.6825 - val_loss: 0.6015\n",
      "Epoch 3/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 302ms/step - accuracy: 0.6817 - loss: 0.5920 - val_accuracy: 0.6945 - val_loss: 0.5974\n",
      "Epoch 4/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 294ms/step - accuracy: 0.7066 - loss: 0.5649 - val_accuracy: 0.6955 - val_loss: 0.5888\n",
      "Epoch 5/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 297ms/step - accuracy: 0.7345 - loss: 0.5288 - val_accuracy: 0.7075 - val_loss: 0.5772\n",
      "Epoch 6/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 300ms/step - accuracy: 0.7501 - loss: 0.5036 - val_accuracy: 0.7140 - val_loss: 0.5630\n",
      "Epoch 7/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 298ms/step - accuracy: 0.7803 - loss: 0.4735 - val_accuracy: 0.7285 - val_loss: 0.5621\n",
      "Epoch 8/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 297ms/step - accuracy: 0.7893 - loss: 0.4495 - val_accuracy: 0.6940 - val_loss: 0.5948\n",
      "Epoch 9/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 297ms/step - accuracy: 0.8069 - loss: 0.4151 - val_accuracy: 0.7125 - val_loss: 0.6300\n",
      "Epoch 10/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 290ms/step - accuracy: 0.8223 - loss: 0.3919 - val_accuracy: 0.7110 - val_loss: 0.6417\n",
      "Epoch 11/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 291ms/step - accuracy: 0.8295 - loss: 0.3836 - val_accuracy: 0.7100 - val_loss: 0.6386\n",
      "Epoch 12/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 295ms/step - accuracy: 0.8434 - loss: 0.3541 - val_accuracy: 0.6940 - val_loss: 0.7483\n",
      "Epoch 13/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 301ms/step - accuracy: 0.8575 - loss: 0.3323 - val_accuracy: 0.7165 - val_loss: 0.6855\n",
      "Epoch 14/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 294ms/step - accuracy: 0.8799 - loss: 0.2850 - val_accuracy: 0.7165 - val_loss: 0.6858\n",
      "Epoch 15/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 293ms/step - accuracy: 0.8860 - loss: 0.2737 - val_accuracy: 0.7260 - val_loss: 0.7378\n",
      "Epoch 16/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 293ms/step - accuracy: 0.8914 - loss: 0.2623 - val_accuracy: 0.7150 - val_loss: 0.7548\n",
      "Epoch 17/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 290ms/step - accuracy: 0.8950 - loss: 0.2496 - val_accuracy: 0.7100 - val_loss: 0.7751\n",
      "Epoch 18/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 292ms/step - accuracy: 0.9083 - loss: 0.2355 - val_accuracy: 0.6970 - val_loss: 0.8438\n",
      "Epoch 19/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 304ms/step - accuracy: 0.9071 - loss: 0.2166 - val_accuracy: 0.7265 - val_loss: 0.9582\n",
      "Epoch 20/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 304ms/step - accuracy: 0.9218 - loss: 0.1954 - val_accuracy: 0.7175 - val_loss: 0.9131\n",
      "Epoch 21/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 302ms/step - accuracy: 0.9231 - loss: 0.1853 - val_accuracy: 0.7255 - val_loss: 0.9465\n",
      "Epoch 22/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 302ms/step - accuracy: 0.9312 - loss: 0.1908 - val_accuracy: 0.7240 - val_loss: 0.9809\n",
      "Epoch 23/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 305ms/step - accuracy: 0.9426 - loss: 0.1604 - val_accuracy: 0.7275 - val_loss: 0.9950\n",
      "Epoch 24/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 323ms/step - accuracy: 0.9376 - loss: 0.1601 - val_accuracy: 0.7070 - val_loss: 1.0198\n",
      "Epoch 25/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 298ms/step - accuracy: 0.9415 - loss: 0.1518 - val_accuracy: 0.7205 - val_loss: 1.0050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1a19d309690>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(x=training_set,validation_data=test_set,epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a995cd1a-3d42-4e81-9a5f-e3a820c0d0f0",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfb5a26-87e9-4ea5-915a-65794023db3a",
   "metadata": {},
   "source": [
    "#making a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b062ea0c-1c7e-46eb-a4a4-7bacd99fb8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "98fc4d25-6b95-4d06-9dbb-a554f43be979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step\n",
      "Dog\n"
     ]
    }
   ],
   "source": [
    "#load the data\n",
    "test_image=Image.open(r\"C:\\Users\\sonia\\Downloads\\single_prediction-20250108T103643Z-001\\single_prediction\\cat_or_dog_1.jpg\")\n",
    "\n",
    "#data preprocessing  \n",
    "test_image= test_image.resize((64,64))\n",
    "test_image=np.array(test_image)\n",
    "test_image=np.expand_dims(test_image,axis=0)\n",
    "\n",
    "#Prediction\n",
    "result=classifier.predict(test_image)\n",
    "\n",
    "#Evalution\n",
    "if result[0][0]==1:\n",
    "   print(\"Dog\")\n",
    "else:\n",
    "   print(\"cat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba15d3f-5e7b-497b-8e15-131579603e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfef53a1-cd3d-4da6-ba11-f926cb023232",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
